{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003caa17-d64f-4cff-89b9-714ff0336959",
   "metadata": {},
   "source": [
    "SCRIPT_NAME: data_analysis.ipynb <p>\n",
    "FUNCTION: Process itron device data <p>\n",
    "AUTHOR: Nioy Chakraborty <p>\n",
    "VERSION: 1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2b61a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import s3fs\n",
    "    import pickle\n",
    "\n",
    "except:\n",
    "    !pip install awswrangler dask s3fs pickle\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5517130",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json =  {\n",
    " \t\"h.rieger\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"15.02.2023T12:40:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"15.02.2023T13:10:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T09:15:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T09:46:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T09:52:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T10:25:00.000000Z \"\n",
    " \t\t}\n",
    " \t},\n",
    " \t\"ck.nielsen\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T13:26:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T13:56:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T14:01:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T14:31:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T14:57:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T15:23:00.000000Z \"\n",
    " \t\t}\n",
    " \t},\n",
    " \t\"j.ruskokivi\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"06.02.2023T15:15:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"06.02.2023T15:45:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"14.02.2023T10:30:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"14.02.2023T11:10:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T12:27:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T13:00:00.000000Z \"\n",
    " \t\t}\n",
    " \t}\n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81dc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket_url = \"s3://grohe-bigdata-itron-sdcard/h.rieger/csv/dump9.csv\"\n",
    "s3_bucket_url_helmut = \"s3://grohe-bigdata-itron-sdcard/h.rieger/csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2770b47-8540-485c-8a8c-360e0d81064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(FILENAME,FILE):\n",
    "    \n",
    "    df = dd.read_csv(FILENAME+FILE.split('/')[-1],sep=';')\n",
    "    \n",
    "    # filter out entries where the number of duplicate entries is less than 4\n",
    "    counts = df.groupby('Time').size()\n",
    "    valid_timestamps = counts[counts == 4].index.compute()\n",
    "    df = df[df['Time'].isin(valid_timestamps)]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['Unnamed: 304','index'],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0124f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_miliseconds(df):\n",
    "    # convert the timestamp column to datetime\n",
    "    df['Time'] = dd.to_datetime(df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # compute the millisecond offset for each row\n",
    "    df['millisecond_offset'] = ((df.index % 4) * 250).astype(str)\n",
    "\n",
    "    df['millisecond_offset'] = df['millisecond_offset'].str.pad(width=3, side='left', fillchar='0')\n",
    "\n",
    "    # combine the date and time components with the millisecond offset to create the new time column\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.') + df['millisecond_offset'].astype(str) + 'Z'\n",
    "\n",
    "    df['Time'] = dd.to_datetime(df['Time'])\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_valid_columns():\n",
    "    capt_up_data = []\n",
    "    capt_down_data = []\n",
    "    dic_measurement = {}\n",
    "    mapping_dict = {}\n",
    "    \n",
    "    Iton_Sensors = ['Time','Tdown/TAD(us)','Tup/TRD(us)','TransitT(us)',\n",
    "                    'DT(us)','Flow(l/h)','uC temp','HSPLL Calib','Count',\n",
    "                    'PGA gain','MaxUp','MaxDown']\n",
    "    \n",
    "    for i in range(0,144):\n",
    "        valup = \"CaptUp\"+str(i)\n",
    "        valdn = \"CaptDn\"+str(i)\n",
    "        capt_up_data.append(valup)\n",
    "        capt_down_data.append(valdn)\n",
    "        \n",
    "    Iton_Sensors.extend(capt_up_data)\n",
    "    Iton_Sensors.extend(capt_down_data)\n",
    "    \n",
    "    dic_measurement['itronSensors'] = Iton_Sensors[1:]\n",
    "    dic_measurement['signalUp'] = capt_up_data\n",
    "    dic_measurement['signalDown'] = capt_down_data\n",
    "    \n",
    "    # Create a mapping dictionary that maps each value in the relevant column to its corresponding key\n",
    "    for key, values in dic_measurement.items():\n",
    "        for value in values:\n",
    "            mapping_dict[value] = key\n",
    "            \n",
    "            \n",
    "    return Iton_Sensors, mapping_dict\n",
    "\n",
    "\n",
    "def create_features(df,mapping_dict,FILENAME):\n",
    "    # Create the 'measurement' column by applying the mapping function to the relevant column in the dataframe\n",
    "    df['_measurement'] = df['level_1'].map(mapping_dict)\n",
    "\n",
    "    # create house ID column from s3 bucket url\n",
    "    df['house_id'] = FILENAME.split('/')[3]\n",
    "    \n",
    "    # Compute the number of unique values in column 'Time'\n",
    "    n = df['Time'].nunique()\n",
    "    \n",
    "    # Create a dictionary that maps each unique value in column 'Time' to an ID\n",
    "    unique_A_values = df['Time'].unique()\n",
    "    id_dict = {value: idx % n for idx, value in enumerate(unique_A_values)}\n",
    "    \n",
    "    # Map the dictionary to column 'Time' to create a new column ID named as 'point'\n",
    "    df['point'] = df['Time'].map(id_dict)\n",
    "    \n",
    "    \n",
    "    df = df.rename(columns = {'level_1':'_field',0:'_value','Time':'_time'})\n",
    "    df_final = df[['point','_measurement','house_id','_time','_field','_value']].copy()\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def create_labels(df,meta_json,FILENAME):\n",
    "    \n",
    "    for i in ['test1','test2','test3']:\n",
    "        \n",
    "        start = meta_json[FILENAME.split('/')[3]][i][\"start_time\"]\n",
    "        end = meta_json[FILENAME.split('/')[3]][i][\"end_time\"]\n",
    "        \n",
    "        start = pd.to_datetime(start)\n",
    "        end = pd.to_datetime(end)\n",
    "        time_range = pd.date_range(start=start, end=end, freq='250ms')\n",
    "\n",
    "        df['_time_match'] = pd.to_datetime(df['_time'])\n",
    "\n",
    "\n",
    "        # Create boolean mask for matching time values\n",
    "        mask = df['_time_match'].isin(time_range)\n",
    "\n",
    "        # Create new column with default value of \"NA\"\n",
    "        df['label'] = 'NA'\n",
    "\n",
    "        # Set value of new column to \"testx\" for matching rows\n",
    "        df.loc[mask, 'label'] = i\n",
    "        \n",
    "        df = df.drop('_time_match',axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c456828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annotated_csv(df):\n",
    "    # Create a list of the annotations to add to the beginning of the file\n",
    "    annotation1 = '#datatype,long,measurement,tag,dateTime:RFC3339,string,double\\n'\n",
    "    \n",
    "    annotation2 = '#group,false,false,true,false,true,false\\n'\n",
    "    \n",
    "\n",
    "    df_string = annotation1 + annotation2 + df.to_csv()\n",
    "\n",
    "    # save the DataFrame as a CSV file\n",
    "    with open('annotated_file.csv', 'w') as f:\n",
    "        f.write(df_string)\n",
    "\n",
    "#ANOTHER WAY 2 \n",
    "#     # Open a new file to write the annotated CSV data\n",
    "#     with open('annotated.csv', 'w') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "\n",
    "#         # Write the annotations to the beginning of the file\n",
    "#         for annotation in annotations:\n",
    "#             writer.writerow([annotation])\n",
    "\n",
    "#         # Write the original CSV data to the file\n",
    "#         for row in df:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# ANOTHER WAY 3\n",
    "# Write the DataFrame to an annotated CSV file\n",
    "# specifying the line_terminator parameter as '\\n' to match the InfluxDB line protocol.\n",
    "\n",
    "# df.to_csv('data_annotated.csv', index=False, header=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085720c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_s3(df,FILE,FILENAME):\n",
    "    # Write the dataframe to a pickle file on S3\n",
    "    fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    bucket_name = FILENAME[:-5]+'/pickle'\n",
    "    file_name = FILE.split('/')[3][:-4]+'.pkl'\n",
    "    print(f'{bucket_name}/{file_name}')\n",
    "#     wr.s3.to_pickle(df, f'{bucket_name}/{file_name}')\n",
    "    pickle.dump(df, fs.open(f'{bucket_name}/{file_name}', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034eaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(FILENAME):\n",
    "    \n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    csv_files = s3.glob(FILENAME+'*.csv')\n",
    "    \n",
    "    for FILE in csv_files[:2]:\n",
    "\n",
    "        #read file from s3 location\n",
    "        df = read_csv(FILENAME,FILE)\n",
    "\n",
    "        # add miliseconds to time and create RFC3339 time format \n",
    "        df = create_miliseconds(df)\n",
    "\n",
    "        # create column names and a mapping dictionary \n",
    "        Iton_Sensors, mapping_dict = create_valid_columns()\n",
    "        df1 = df[Iton_Sensors].copy()\n",
    "\n",
    "        # convert dask dataframe to pandas dataframe\n",
    "        df1 = df1.compute()\n",
    "\n",
    "        # reformat dataframe\n",
    "        df1 = df1.set_index('Time')\n",
    "        dfx = df1.stack().reset_index()\n",
    "\n",
    "        # create additional features\n",
    "        df_final = create_features(dfx,mapping_dict,FILENAME)\n",
    "\n",
    "        # add annotation line at the top and save the file\n",
    "#         write_annotated_csv(df_final)\n",
    "\n",
    "        # create labels\n",
    "        dfx = create_labels(df_final,meta_json,FILENAME)\n",
    "        \n",
    "        write_s3(dfx,FILE,FILENAME)\n",
    "#         print(dfx.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca82b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://grohe-bigdata-itron-sdcard/h.rieger/pickle/dump0.pkl\n"
     ]
    },
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/aiobotocore/response.py:57\u001b[0m, in \u001b[0;36mStreamingBody.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m     58\u001b[0m         amt \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/aiohttp/streams.py:375\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadany()\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/aiohttp/streams.py:397\u001b[0m, in \u001b[0;36mStreamReader.readany\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadany\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_nowait(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/aiohttp/streams.py:304\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mServerTimeoutError\u001b[0m: Timeout on reading data from socket",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAioReadTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/fsspec/asyn.py:54\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/s3fs/core.py:2298\u001b[0m, in \u001b[0;36m_inner_fetch\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2290\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fs\u001b[38;5;241m.\u001b[39m_call_s3(\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2292\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreq_kw,\n\u001b[1;32m   2297\u001b[0m )\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/aiobotocore/response.py:61\u001b[0m, in \u001b[0;36mStreamingBody.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AioReadTimeoutError(\n\u001b[1;32m     62\u001b[0m         endpoint_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__\u001b[38;5;241m.\u001b[39murl, error\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientConnectionError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAioReadTimeoutError\u001b[0m: Read timeout on endpoint URL: \"https://grohe-bigdata-itron-sdcard.s3.eu-central-1.amazonaws.com/h.rieger/csv/dump1.csv\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_bucket_url_helmut\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [8], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(FILENAME)\u001b[0m\n\u001b[1;32m      4\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mglob(FILENAME\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m FILE \u001b[38;5;129;01min\u001b[39;00m csv_files[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#read file from s3 location\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILENAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# add miliseconds to time and create RFC3339 time format \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     df \u001b[38;5;241m=\u001b[39m create_miliseconds(df)\n",
      "Cell \u001b[0;32mIn [4], line 7\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(FILENAME, FILE)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# filter out entries where the number of duplicate entries is less than 4\u001b[39;00m\n\u001b[1;32m      6\u001b[0m counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m----> 7\u001b[0m valid_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(valid_timestamps)]\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/core.py:113\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"Do the actual work of collecting data and executing a function\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m'foo'\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m istask(arg):\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/core.py:113\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"Do the actual work of collecting data and executing a function\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m'foo'\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m istask(arg):\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/dask/bytes/core.py:192\u001b[0m, in \u001b[0;36mread_block_from_file\u001b[0;34m(lazy_file, off, bs, delimiter)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m off \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m bs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/fsspec/utils.py:266\u001b[0m, in \u001b[0;36mread_block\u001b[0;34m(f, offset, length, delimiter, split_before)\u001b[0m\n\u001b[1;32m    263\u001b[0m     length \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    265\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(offset)\n\u001b[0;32m--> 266\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m b\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/fsspec/spec.py:1684\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1682\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1684\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/fsspec/caching.py:154\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    152\u001b[0m     part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/s3fs/core.py:2143\u001b[0m, in \u001b[0;36mS3File._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch_range\u001b[39m(\u001b[38;5;28mself\u001b[39m, start, end):\n\u001b[1;32m   2142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreq_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINVAL \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre-conditions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/s3fs/core.py:2286\u001b[0m, in \u001b[0;36m_fetch_range\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2285\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetch: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, start, end)\n\u001b[0;32m-> 2286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_inner_fetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/fsspec/asyn.py:97\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m return_result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[0;31mFSTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "if __name__ == \"__main__\":\n",
    "    main(s3_bucket_url_helmut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7097d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
