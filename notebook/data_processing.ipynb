{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003caa17-d64f-4cff-89b9-714ff0336959",
   "metadata": {},
   "source": [
    "SCRIPT_NAME: data_analysis.ipynb <p>\n",
    "FUNCTION: Process itron device data <p>\n",
    "AUTHOR: Nioy Chakraborty <p>\n",
    "VERSION: 1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2b61a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import s3fs\n",
    "    import pickle\n",
    "    import boto3\n",
    "    import tqdm\n",
    "    import os, time\n",
    "\n",
    "except:\n",
    "    !pip install awswrangler dask boto3 pickle\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5517130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_json =  {\n",
    " \t\"h.rieger\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"15.02.2023T12:40:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"15.02.2023T13:10:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T09:15:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T09:46:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T09:52:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T10:25:00.000000Z \"\n",
    " \t\t}\n",
    " \t},\n",
    " \t\"ck.nielsen\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T13:26:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T13:56:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T14:01:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T14:31:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"23.02.2023T14:57:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"23.02.2023T15:23:00.000000Z \"\n",
    " \t\t}\n",
    " \t},\n",
    " \t\"j.ruskokivi\": {\n",
    " \t\t\"test1\": {\n",
    " \t\t\t\"start_time\": \"06.02.2023T15:15:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"06.02.2023T15:45:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test2\": {\n",
    " \t\t\t\"start_time\": \"14.02.2023T10:30:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"14.02.2023T11:10:00.000000Z \"\n",
    " \t\t},\n",
    " \t\t\"test3\": {\n",
    " \t\t\t\"start_time\": \"20.02.2023T12:27:00.000000Z \",\n",
    " \t\t\t\"end_time\": \"20.02.2023T13:00:00.000000Z \"\n",
    " \t\t}\n",
    " \t}\n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81dc918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s3_bucket_url = \"s3://grohe-bigdata-itron-sdcard/h.rieger/csv/dump9.csv\"\n",
    "s3_bucket_url_helmut = \"s3://grohe-bigdata-itron-sdcard/h.rieger/csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2770b47-8540-485c-8a8c-360e0d81064c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_csv(FILENAME,FILE):\n",
    "    \n",
    "    df = dd.read_csv(FILENAME+FILE.split('/')[-1],sep=';')\n",
    "    \n",
    "    # filter out entries where the number of duplicate entries is less than 4\n",
    "    counts = df.groupby('Time').size()\n",
    "    valid_timestamps = counts[counts == 4].index.compute()\n",
    "    df = df[df['Time'].isin(valid_timestamps)]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['Unnamed: 304','index'],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0124f33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_miliseconds(df):\n",
    "    # convert the timestamp column to datetime\n",
    "    df['Time'] = dd.to_datetime(df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # compute the millisecond offset for each row\n",
    "    df['millisecond_offset'] = ((df.index % 4) * 250).astype(str)\n",
    "\n",
    "    df['millisecond_offset'] = df['millisecond_offset'].str.pad(width=3, side='left', fillchar='0')\n",
    "\n",
    "    # combine the date and time components with the millisecond offset to create the new time column\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.') + df['millisecond_offset'].astype(str) + 'Z'\n",
    "\n",
    "    df['Time'] = dd.to_datetime(df['Time'])\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_valid_columns():\n",
    "    capt_up_data = []\n",
    "    capt_down_data = []\n",
    "    dic_measurement = {}\n",
    "    mapping_dict = {}\n",
    "    \n",
    "    Iton_Sensors = ['Time','Tdown/TAD(us)','Tup/TRD(us)','TransitT(us)',\n",
    "                    'DT(us)','Flow(l/h)','uC temp','HSPLL Calib','Count',\n",
    "                    'PGA gain','MaxUp','MaxDown']\n",
    "    \n",
    "    for i in range(0,144):\n",
    "        valup = \"CaptUp\"+str(i)\n",
    "        valdn = \"CaptDn\"+str(i)\n",
    "        capt_up_data.append(valup)\n",
    "        capt_down_data.append(valdn)\n",
    "        \n",
    "    Iton_Sensors.extend(capt_up_data)\n",
    "    Iton_Sensors.extend(capt_down_data)\n",
    "    \n",
    "    dic_measurement['itronSensors'] = Iton_Sensors[1:]\n",
    "    dic_measurement['signalUp'] = capt_up_data\n",
    "    dic_measurement['signalDown'] = capt_down_data\n",
    "    \n",
    "    # Create a mapping dictionary that maps each value in the relevant column to its corresponding key\n",
    "    for key, values in dic_measurement.items():\n",
    "        for value in values:\n",
    "            mapping_dict[value] = key\n",
    "            \n",
    "            \n",
    "    return Iton_Sensors, mapping_dict\n",
    "\n",
    "\n",
    "def create_features(df,mapping_dict,FILENAME):\n",
    "    # Create the 'measurement' column by applying the mapping function to the relevant column in the dataframe\n",
    "    df['_measurement'] = df['level_1'].map(mapping_dict)\n",
    "\n",
    "    # create house ID column from s3 bucket url\n",
    "    df['house_id'] = FILENAME.split('/')[3]\n",
    "    \n",
    "    # Compute the number of unique values in column 'Time'\n",
    "    n = df['Time'].nunique()\n",
    "    \n",
    "    # Create a dictionary that maps each unique value in column 'Time' to an ID\n",
    "    unique_A_values = df['Time'].unique()\n",
    "    id_dict = {value: idx % n for idx, value in enumerate(unique_A_values)}\n",
    "    \n",
    "    # Map the dictionary to column 'Time' to create a new column ID named as 'point'\n",
    "    df['point'] = df['Time'].map(id_dict)\n",
    "    \n",
    "    \n",
    "    df = df.rename(columns = {'level_1':'_field',0:'_value','Time':'_time'})\n",
    "    df_final = df[['point','_measurement','house_id','_time','_field','_value']].copy()\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def create_labels(df,meta_json,FILENAME):\n",
    "    \n",
    "    for i in ['test1','test2','test3']:\n",
    "        \n",
    "        start = meta_json[FILENAME.split('/')[3]][i][\"start_time\"]\n",
    "        end = meta_json[FILENAME.split('/')[3]][i][\"end_time\"]\n",
    "        \n",
    "        start = pd.to_datetime(start)\n",
    "        end = pd.to_datetime(end)\n",
    "        time_range = pd.date_range(start=start, end=end, freq='250ms')\n",
    "\n",
    "        df['_time_match'] = pd.to_datetime(df['_time'])\n",
    "\n",
    "\n",
    "        # Create boolean mask for matching time values\n",
    "        mask = df['_time_match'].isin(time_range)\n",
    "\n",
    "        # Create new column with default value of \"NA\"\n",
    "        df['label'] = 'NA'\n",
    "\n",
    "        # Set value of new column to \"testx\" for matching rows\n",
    "        df.loc[mask, 'label'] = i\n",
    "        \n",
    "        df = df.drop('_time_match',axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c456828c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_annotated_csv(df):\n",
    "    # Create a list of the annotations to add to the beginning of the file\n",
    "    annotation1 = '#datatype,long,measurement,tag,dateTime:RFC3339,string,double,tag\\n'\n",
    "    \n",
    "    annotation2 = '#group,false,false,true,false,true,false,true\\n'\n",
    "    \n",
    "\n",
    "    df_string = annotation1 + annotation2 + df.to_csv(index=False)\n",
    "    \n",
    "    return df_string\n",
    "    # save the DataFrame as a CSV file\n",
    "    # with open('annotated_file.csv', 'w') as f:\n",
    "    #     f.write(df_string)\n",
    "\n",
    "#ANOTHER WAY 2 \n",
    "#     # Open a new file to write the annotated CSV data\n",
    "#     with open('annotated.csv', 'w') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "\n",
    "#         # Write the annotations to the beginning of the file\n",
    "#         for annotation in annotations:\n",
    "#             writer.writerow([annotation])\n",
    "\n",
    "#         # Write the original CSV data to the file\n",
    "#         for row in df:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# ANOTHER WAY 3\n",
    "# Write the DataFrame to an annotated CSV file\n",
    "# specifying the line_terminator parameter as '\\n' to match the InfluxDB line protocol.\n",
    "\n",
    "# df.to_csv('data_annotated.csv', index=False, header=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085720c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_s3(df,FILE,FILENAME):\n",
    "    # Write the dataframe to a pickle file on S3\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    annotation1 = '#datatype,long,measurement,tag,dateTime:RFC3339,string,double,tag\\n'\n",
    "    annotation2 = '#group,false,false,true,false,true,false,true\\n'\n",
    "\n",
    "    num_rows = df.shape[0]\n",
    "    chunk_size = 500000\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, num_rows, chunk_size)):\n",
    "\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        df_string = annotation1 + annotation2 + chunk.to_csv(index=False)\n",
    "\n",
    "        filename = 'annotated_file_'+str(i)+'.csv'\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(df_string)\n",
    "\n",
    "        os.system(\"influx write --bucket testData --format=csv --file ./\"+filename)\n",
    "        os.remove(filename)\n",
    "        \n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034eaad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(FILENAME):\n",
    "    \n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    csv_files = s3.glob(FILENAME+'*.csv')\n",
    "    \n",
    "    for FILE in csv_files[:2]:\n",
    "\n",
    "        #read file from s3 location\n",
    "        df = read_csv(FILENAME,FILE)\n",
    "\n",
    "        # add miliseconds to time and create RFC3339 time format \n",
    "        df = create_miliseconds(df)\n",
    "\n",
    "        # create column names and a mapping dictionary \n",
    "        Iton_Sensors, mapping_dict = create_valid_columns()\n",
    "        df1 = df[Iton_Sensors].copy()\n",
    "\n",
    "        # convert dask dataframe to pandas dataframe\n",
    "        df1 = df1.compute()\n",
    "\n",
    "        # reformat dataframe\n",
    "        df1 = df1.set_index('Time')\n",
    "        dfx = df1.stack().reset_index()\n",
    "\n",
    "        # create additional features\n",
    "        df_final = create_features(dfx,mapping_dict,FILENAME)\n",
    "\n",
    "        # create labels\n",
    "        dfx = create_labels(df_final,meta_json,FILENAME)\n",
    "        \n",
    "        # add annotation line at the top and save the file\n",
    "        # dfx = write_annotated_csv(df_final)\n",
    "        \n",
    "    dfx.insert(0, '', '')\n",
    "    # write_s3(dfx,FILE,FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca82b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['', 'point', '_measurement', 'house_id', '_time', '_field', '_value',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "if __name__ == \"__main__\":\n",
    "    main(s3_bucket_url_helmut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31051b75-4fbd-41a7-adab-fcbb68efc2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
