{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003caa17-d64f-4cff-89b9-714ff0336959",
   "metadata": {},
   "source": [
    "SCRIPT_NAME: data_analysis.ipynb <p>\n",
    "FUCTION: Process itron device data <p>\n",
    "AUTHOR: Nioy Chakraborty <p>\n",
    "VERSION: 1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2b61a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import s3fs\n",
    "except:\n",
    "    !pip install awswrangler dask s3fs\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import dask.dataframe as dd\n",
    "    import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b843be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = {\n",
    "\t\"house1\": {\n",
    "\t\t\"owner_name\": \"h.rieger\",\n",
    "\t\t\"test_times\": {\n",
    "\t\t\t\"test1\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test2\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test3\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test4\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"house2\": {\n",
    "\t\t\"owner_name\": \"claus\",\n",
    "\t\t\"test_times\": {\n",
    "\t\t\t\"test1\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test2\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test3\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test4\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"house3\": {\n",
    "\t\t\"owner_name\": \"juho\",\n",
    "\t\t\"test_times\": {\n",
    "\t\t\t\"test1\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test2\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test3\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"test4\": {\n",
    "\t\t\t\t\"date\": \"\",\n",
    "\t\t\t\t\"start_time\": \"\",\n",
    "\t\t\t\t\"end_time\": \"\"\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81dc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_url = \"s3://grohe-bigdata-itron-sdcard/h.rieger/csv/dump9.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2770b47-8540-485c-8a8c-360e0d81064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(FILENAME):\n",
    "    df = dd.read_csv(FILENAME,sep=';')\n",
    "    \n",
    "    # filter out entries where the number of duplicate entries is less than 4\n",
    "    counts = df.groupby('Time').size()\n",
    "    valid_timestamps = counts[counts == 4].index.compute()\n",
    "    df = df[df['Time'].isin(valid_timestamps)]\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['Unnamed: 304','index'],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_miliseconds(df):\n",
    "    # convert the timestamp column to datetime\n",
    "    df['Time'] = dd.to_datetime(df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # compute the millisecond offset for each row\n",
    "    df['millisecond_offset'] = ((df.index % 4) * 250).astype(str)\n",
    "\n",
    "    df['millisecond_offset'] = df['millisecond_offset'].str.pad(width=3, side='left', fillchar='0')\n",
    "\n",
    "    # combine the date and time components with the millisecond offset to create the new time column\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.') + df['millisecond_offset'].astype(str) + 'Z'\n",
    "\n",
    "    df['Time'] = dd.to_datetime(df['Time'])\n",
    "    df['Time'] = df['Time'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_valid_columns():\n",
    "    capt_up_data = []\n",
    "    capt_down_data = []\n",
    "    dic_measurement = {}\n",
    "    mapping_dict = {}\n",
    "    \n",
    "    Iton_Sensors = ['Time','Tdown/TAD(us)','Tup/TRD(us)','TransitT(us)',\n",
    "                    'DT(us)','Flow(l/h)','uC temp','HSPLL Calib','Count',\n",
    "                    'PGA gain','MaxUp','MaxDown']\n",
    "    \n",
    "    for i in range(0,144):\n",
    "        valup = \"CaptUp\"+str(i)\n",
    "        valdn = \"CaptDn\"+str(i)\n",
    "        capt_up_data.append(valup)\n",
    "        capt_down_data.append(valdn)\n",
    "        \n",
    "    Iton_Sensors.extend(capt_up_data)\n",
    "    Iton_Sensors.extend(capt_down_data)\n",
    "    \n",
    "    dic_measurement['itronSensors'] = Iton_Sensors[1:]\n",
    "    dic_measurement['signalUp'] = capt_up_data\n",
    "    dic_measurement['signalDown'] = capt_down_data\n",
    "    \n",
    "    # Create a mapping dictionary that maps each value in the relevant column to its corresponding key\n",
    "    for key, values in dic_measurement.items():\n",
    "        for value in values:\n",
    "            mapping_dict[value] = key\n",
    "            \n",
    "            \n",
    "    return Iton_Sensors, mapping_dict\n",
    "\n",
    "\n",
    "def create_features(df,mapping_dict,FILENAME):\n",
    "    # Create the 'measurement' column by applying the mapping function to the relevant column in the dataframe\n",
    "    df['_measurement'] = df['level_1'].map(mapping_dict)\n",
    "\n",
    "    # create house ID column from s3 bucket url\n",
    "    df['house_id'] = FILENAME.split('/')[3]\n",
    "    \n",
    "    # Compute the number of unique values in column 'Time'\n",
    "    n = df['Time'].nunique()\n",
    "    \n",
    "    # Create a dictionary that maps each unique value in column 'Time' to an ID\n",
    "    unique_A_values = df['Time'].unique()\n",
    "    id_dict = {value: idx % n for idx, value in enumerate(unique_A_values)}\n",
    "    \n",
    "    # Map the dictionary to column 'Time' to create a new column ID named as 'point'\n",
    "    df['point'] = df['Time'].map(id_dict)\n",
    "    \n",
    "    \n",
    "    df = df.rename(columns = {'level_1':'_field',0:'_value','Time':'_time'})\n",
    "    df_final = df[['point','_measurement','house_id','_time','_field','_value']].copy()\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def write_annotated_csv(df):\n",
    "    # Create a list of the annotations to add to the beginning of the file\n",
    "    annotation1 = '#datatype,long,measurement,tag,dateTime:RFC3339,string,double\\n'\n",
    "    \n",
    "    annotation2 = '#group,false,false,true,false,true,false\\n'\n",
    "    \n",
    "\n",
    "    df_string = annotation1 + annotation2 + df.to_csv()\n",
    "\n",
    "    # save the DataFrame as a CSV file\n",
    "    with open('annotated_file.csv', 'w') as f:\n",
    "        f.write(df_string)\n",
    "\n",
    "#ANOTHER WAY 2 \n",
    "#     # Open a new file to write the annotated CSV data\n",
    "#     with open('annotated.csv', 'w') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "\n",
    "#         # Write the annotations to the beginning of the file\n",
    "#         for annotation in annotations:\n",
    "#             writer.writerow([annotation])\n",
    "\n",
    "#         # Write the original CSV data to the file\n",
    "#         for row in df:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# ANOTHER WAY 3\n",
    "# Write the DataFrame to an annotated CSV file\n",
    "# specifying the line_terminator parameter as '\\n' to match the InfluxDB line protocol.\n",
    "\n",
    "# df.to_csv('data_annotated.csv', index=False, header=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "034eaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(FILENAME):\n",
    "    # read file from s3 location\n",
    "    df = read_csv(FILENAME)\n",
    "    \n",
    "    # add miliseconds to time and create RFC3339 time format \n",
    "    df = create_miliseconds(df)\n",
    "    \n",
    "    # create column names and a mapping dictionary \n",
    "    Iton_Sensors, mapping_dict = create_valid_columns()\n",
    "    df1 = df[Iton_Sensors].copy()\n",
    "    \n",
    "    # convert dask dataframe to pandas dataframe\n",
    "    df1 = df1.compute()\n",
    "    \n",
    "    # reformat dataframe\n",
    "    df1 = df1.set_index('Time')\n",
    "    dfx = df1.stack().reset_index()\n",
    "\n",
    "    # create additional features\n",
    "    df_final = create_features(dfx,mapping_dict,FILENAME)\n",
    "\n",
    "    # add annotation line at the top and save the file\n",
    "    write_annotated_csv(df_final)\n",
    "    \n",
    "    print(df_final.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca82b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point  _measurement  house_id                        _time         _field  \\\n",
      "0      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z  Tdown/TAD(us)   \n",
      "1      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z    Tup/TRD(us)   \n",
      "2      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z   TransitT(us)   \n",
      "3      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z         DT(us)   \n",
      "4      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z      Flow(l/h)   \n",
      "5      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z        uC temp   \n",
      "6      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z    HSPLL Calib   \n",
      "7      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z          Count   \n",
      "8      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z       PGA gain   \n",
      "9      0  itronSensors  h.rieger  2023-02-01T13:13:01.000000Z          MaxUp   \n",
      "\n",
      "       _value  \n",
      "0   52.792760  \n",
      "1   52.828160  \n",
      "2    0.000000  \n",
      "3    0.042942  \n",
      "4  210.430400  \n",
      "5    8.300000  \n",
      "6    1.005621  \n",
      "7    8.000000  \n",
      "8   26.000000  \n",
      "9  473.871800  \n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "if __name__ == \"__main__\":\n",
    "    main(s3_bucket_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
